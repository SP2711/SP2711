# -*- coding: utf-8 -*-
"""old code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YK209rxYBiKG_oNozXvUt8BGtLPv81KI
"""

import pandas as pd
import numpy as np
import re
import string
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab') # Download the required 'punkt_tab' data package
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from google.colab import files
uploaded = files.upload()

df=pd.read_csv("datasetset.csv")
df

df.head()

print("\nMissing Values:")
print(df.isnull().sum())

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Initialize tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Preprocessing function
def clean_text(text):
    if isinstance(text, str):
        text = text.lower()  # Convert to lowercase
        text = re.sub(r'\d+', '', text)  # Remove numbers
        text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
        tokens = word_tokenize(text)  # Tokenize
        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Remove stopwords & Lemmatize
        return " ".join(tokens)
    return ""

print(df.columns)

df.columns = df.columns.str.strip()  # Remove any extra spaces
df.rename(columns={"Column 2": "text", "Column 3": "label", "Column 4": "Pattern Category"}, inplace=True)

# Verify the updated column names
print(df.columns)

df['cleaned_text'] = df['text'].astype(str).apply(clean_text)
print(df[['text', 'cleaned_text']].head())  # Check results

print(df.head())  # See the first few rows of the dataset

#Step 6: Convert Labels into Numerical Format
label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['label'])  # Convert labels to numbers

df

# Step 8: Convert Text into Numerical Features (TF-IDF)
# Assuming 'cleaned_text' column contains your text data and 'label_encoded' contains your target labels
from sklearn.model_selection import train_test_split # Import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label_encoded'], test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 9: Display Final Shapes
print(f"\nTrain Shape: {X_train_tfidf.shape}, Test Shape: {X_test_tfidf.shape}")
print("âœ… Preprocessing Complete!")

# Step 1: Import necessary libraries
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Step 2: Train the SVM Model
svm_model = SVC(kernel='linear', C=1.0)  # Linear Kernel for text classification
svm_model.fit(X_train_tfidf, y_train)

#  Step 3: Predict on Test Data
y_pred = svm_model.predict(X_test_tfidf)

#  Step 4: Evaluate Model Performance
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Model Accuracy: {accuracy:.4f}")
print("\nðŸ”¹ Classification Report:")
print(classification_report(y_test, y_pred))

# Step 1: Import Required Libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

#Step 4: Convert Text to Numerical Data (TF-IDF)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X = vectorizer.fit_transform(df['text'])  # Transform text into TF-IDF matrix
y = df['label']  # Target variable

# Step 5: Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train Logistic Regression Model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

#  Step 7: Predict on Test Data
y_pred = logistic_model.predict(X_test)

#  Step 8: Evaluate Model Performance
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Logistic Regression Accuracy: {accuracy:.4f}\n")

print("ðŸ”¹ Classification Report:")
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

# 7. Evaluate Models
print("Random Forest Model:")
print(f"Accuracy: {accuracy_score(y_test, rf_pred):.4f}")
print(classification_report(y_test, rf_pred))

#DECISION TREE
from sklearn.tree import DecisionTreeClassifier

# Step 6: Train Decision Tree Model
dt_model = DecisionTreeClassifier(random_state=42)  # You can adjust hyperparameters here
dt_model.fit(X_train, y_train)

# Step 7: Predict on Test Data
dt_pred = dt_model.predict(X_test)

# Step 8: Evaluate the Decision Tree Model
print("\nDecision Tree Model:")
print(f"Accuracy: {accuracy_score(y_test, dt_pred):.4f}")
print(classification_report(y_test, dt_pred))

# K-NEAREST NEIGHBORS (KNN)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 6: Train KNN Model
knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust 'n_neighbors' for tuning
knn_model.fit(X_train_tfidf, y_train)

# Step 7: Predict on Test Data
knn_pred = knn_model.predict(X_test_tfidf)

# Step 8: Evaluate the KNN Model
print("\nKNN Model:")
print(f"Accuracy: {accuracy_score(y_test, knn_pred):.4f}")
print(classification_report(y_test, knn_pred))

#POLYNOMIAL REGESSION

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score

# Assuming 'df' is your DataFrame and you've already preprocessed it
# If not, please include your preprocessing steps here.

# Example: Let's say 'text_length' is a numerical feature you want to use
df['text_length'] = df['text'].astype(str).apply(len)

# Select your features (X) and target variable (y)
X = df[['text_length']] # Example: Using text length as the feature
y = df['label_encoded'] # Example: Using label_encoded as target.  Replace with your desired target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create polynomial features
poly = PolynomialFeatures(degree=2) # You can adjust the degree as needed
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Train a linear regression model on the polynomial features
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_poly)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# NAIVE BAYES
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Step 6: Train Naive Bayes Model
nb_model = MultinomialNB()  # Best suited for text classification
nb_model.fit(X_train_tfidf, y_train)

# Step 7: Predict on Test Data
nb_pred = nb_model.predict(X_test_tfidf)

# Step 8: Evaluate the Naive Bayes Model
print("\nNaive Bayes Model:")
print(f"Accuracy: {accuracy_score(y_test, nb_pred):.4f}")
print(classification_report(y_test, nb_pred))

#NEURAL NETWORK

import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Assuming X_train_tfidf, X_test_tfidf, y_train, y_test are already defined from the previous code

# Define the model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train_tfidf.shape[1],)),
    keras.layers.Dropout(0.5),  # Add dropout for regularization
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(len(np.unique(y_train)), activation='softmax') # Output layer with softmax for multi-class classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', # Use sparse categorical crossentropy for integer labels
              metrics=['accuracy'])

# Train the model
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_split=0.2) # Adjust epochs and batch size as needed


# Evaluate the model
loss, accuracy = model.evaluate(X_test_tfidf, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Make predictions
y_pred_prob = model.predict(X_test_tfidf)
y_pred = np.argmax(y_pred_prob, axis=1) # Get the class with highest probability


from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

# ANN

import pandas as pd
import numpy as np
import re
import string
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from nltk.corpus import stopwords
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import classification_report

# Download required NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

# Load the dataset (assuming 'datasetset.csv' is in the current directory or provide the correct path)
try:
    df = pd.read_csv("datasetset.csv")
except FileNotFoundError:
    print("Error: 'datasetset.csv' not found. Please upload the file or provide the correct path.")
    # You might want to exit or handle the error differently here
    exit()


# Preprocessing (assuming the preprocessing steps from the original code are correct and complete)
def clean_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = re.sub(r'\d+', '', text)
        text = text.translate(str.maketrans('', '', string.punctuation))
        tokens = nltk.word_tokenize(text)
        tokens = [word for word in tokens if word not in stopwords.words('english')]
        return " ".join(tokens)
    else:
        return ""

df.columns = df.columns.str.strip()
df.rename(columns={"Column 2": "text", "Column 3": "label", "Column 4": "Pattern Category"}, inplace=True)
df['cleaned_text'] = df['text'].astype(str).apply(clean_text)

label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['label'])

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    df['cleaned_text'], df['label_encoded'], test_size=0.2, random_state=42)

# Vectorize text data
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Define the ANN model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train_tfidf.shape[1],)),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(len(np.unique(y_train)), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_tfidf, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Make predictions and print classification report
y_pred_prob = model.predict(X_test_tfidf)
y_pred = np.argmax(y_pred_prob, axis=1)
print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# KNN Hyperparameter Tuning
knn = KNeighborsClassifier()
knn_param_grid = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}
knn_grid = GridSearchCV(knn, knn_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
knn_grid.fit(X_train_tfidf, y_train)

# Best parameters and evaluation
best_knn = knn_grid.best_estimator_
y_pred = best_knn.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print("Best KNN Parameters:", knn_grid.best_params_)
print("KNN Accuracy (after tuning):", accuracy)

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# SVM Hyperparameter Tuning
svm = SVC()
svm_param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
svm_grid = GridSearchCV(svm, svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
svm_grid.fit(X_train_tfidf, y_train)

# Best parameters and evaluation
best_svm = svm_grid.best_estimator_
y_pred = best_svm.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print("Best SVM Parameters:", svm_grid.best_params_)
print("SVM Accuracy (after tuning):", accuracy)

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Decision Tree Hyperparameter Tuning
dt = DecisionTreeClassifier(random_state=42)
dt_param_grid = {'max_depth': [5, 10, 20], 'criterion': ['gini', 'entropy']}
dt_grid = GridSearchCV(dt, dt_param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Use the TF-IDF vectorized data (X_train_tfidf) instead of raw text (X_train)
dt_grid.fit(X_train_tfidf, y_train)

# Best parameters and evaluation
best_dt = dt_grid.best_estimator_
# Predict using the TF-IDF vectorized test data (X_test_tfidf)
y_pred = best_dt.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print("Best Decision Tree Parameters:", dt_grid.best_params_)
print("Decision Tree Accuracy (after tuning):", accuracy)

from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# NaÃ¯ve Bayes Hyperparameter Tuning
nb = MultinomialNB()
nb_param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}
nb_grid = GridSearchCV(nb, nb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
nb_grid.fit(X_train_tfidf, y_train)

# Best parameters and evaluation
best_nb = nb_grid.best_estimator_
y_pred = best_nb.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print("Best NaÃ¯ve Bayes Parameters:", nb_grid.best_params_)
print("NaÃ¯ve Bayes Accuracy (after tuning):", accuracy)

from sklearn.ensemble import RandomForestClassifier

rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20], 'criterion': ['gini', 'entropy']}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
# Use X_train_tfidf for fitting (TF-IDF features)
rf_grid.fit(X_train_tfidf, y_train)

print("Best Random Forest Parameters:", rf_grid.best_params_)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

models = {'KNN': best_knn, 'SVM': best_svm, 'Decision Tree': best_dt, 'NaÃ¯ve Bayes': best_nb}
for name, model in models.items():
  # Use X_test_tfidf for all models, including Decision Tree
  y_pred = model.predict(X_test_tfidf)
  print(f"\n{name} Classification Report:\n", classification_report(y_test, y_pred))
  print(f"\n{name} Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

accuracies = {name: accuracy_score(y_test, model.predict(X_test_tfidf)) for name, model in models.items()} # Use X_test_tfidf for all models, including Decision Tree
print("Model Performance Comparison:", accuracies)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame and 'label' is the column with true labels
# y_true should be from your test set, not the entire dataset
y_true = y_test  # y_test contains the true labels for the test set
best_model = best_svm
# Ensure y_pred is also from the same test set prediction:
# Assuming 'best_model' is your best performing model
y_pred = best_model.predict(X_test_tfidf)  # Re-generate predictions for the test set

# Create confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Dark Pattern', 'Dark Pattern'],
            yticklabels=['Not Dark Pattern', 'Dark Pattern'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Calculate text lengths
df['text_length'] = df['text'].apply(len)

# Plot distribution
plt.figure(figsize=(10, 6))
sns.boxplot(x='label', y='text_length', data=df)
plt.title('Distribution of Text Lengths by Pattern Type')
plt.xlabel('Pattern Type (0=Not Dark, 1=Dark)')
plt.ylabel('Text Length (characters)')
plt.show()

# Filter only dark patterns (label=1)
dark_patterns = df[df['label'] == '1']

# Count pattern categories
pattern_counts = dark_patterns['Pattern Category'].value_counts()

# Plot frequency distribution
plt.figure(figsize=(12, 6))
pattern_counts.plot(kind='bar', color='skyblue')
plt.title('Frequency Distribution of Dark Pattern Categories')
plt.xlabel('Pattern Category')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import joblib

# Load the trained SVM model
loaded_model = joblib.load('/content/best_dark_pattern_model.pkl')

# Save the TF-IDF vectorizer first (if not already saved)
# Make sure to run this cell before loading the vectorizer
joblib.dump(vectorizer, '/content/tfidf_vectorizer.pkl')

# Load the saved TF-IDF vectorizer
tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')

# Assuming your text column is named 'text'
new_data = df['text'].astype(str)  # Convert to string

# Apply the same TF-IDF transformation
new_data_tfidf = tfidf_vectorizer.transform(new_data)

# Predict dark pattern classifications
predictions = loaded_model.predict(new_data_tfidf)

# Add predictions to the DataFrame
df['Predicted_Class'] = predictions

# Save the updated file
df.to_csv('/content/predictions_output.csv', index=False)

print("Predictions saved as 'predictions_output.csv'")

from google.colab import files

files.download('/content/predictions_output.csv')

from sklearn.metrics import accuracy_score, classification_report

# Extract actual and predicted labels
y_true = df['label']  # Actual labels from your CSV
y_pred = df['Predicted_Class']  # Model predictions

# Calculate accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

# Detailed classification report
print("Classification Report:")
print(classification_report(y_true, y_pred))

incorrect_preds = df[df['label'] != df['Predicted_Class']]
print("Misclassified Examples:")
print(incorrect_preds.head(10))

incorrect_preds = df[df['label'] != df['Predicted_Class']]
print("Incorrect Predictions:")
print(incorrect_preds)

from sklearn.metrics import classification_report, accuracy_score

# Assuming 'df' is the DataFrame containing your new data with predictions
y_true = df['label']  # Replace 'label' with the actual column name for true labels
y_pred = df['Predicted_Class']  # Replace 'Predicted_Class' with the actual column name for predictions

print("Accuracy:", accuracy_score(y_true, y_pred))
print(classification_report(y_true, y_pred))

